/**
 * ===========================
 * GEMmaker Configuration File
 * ===========================
 *
 * This file provides the configuration settings for the GEMmaker workflow.
 */
manifest {
  mainScript = "main.nf"
  defaultBranch = "master"
  nextflowVersion = ">=0.32.0"
}

params {
  input {
    remote_list_path = "none"
    local_samples_path = "${PWD}/examples/LocalRunExample/Data/Sample*/*_{1,2}.fastq"
    reference_path = "${PWD}/examples/LocalRunExample/reference/"
    reference_prefix = "CORG"
  }

  output {
    dir = "${PWD}/output"
    sample_dir = { "${params.output.dir}/${sample_id}" }
    publish_mode = "link"
  }

  // TODO: Consider moving to profiles.
  execution {
    queue_size = 100
    threads = 1
  }

  // TODO: Consider moving to profiles.
  software {
    trimmomatic {
      clip_path = "${PWD}/files/fasta_adapter.txt"
      MINLEN = "0.7"
      quality = ""
      SLIDINGWINDOW = "4:15"
      LEADING = "3"
      TRAILING = "6"
    }
    fpkm_or_tpm {
      fpkm = true
      tpm = true
    }
  }
}


report {
  file = "${params.output.dir}/report.html"
}

timeline {
  file = "${params.output.dir}/timeline.html"
}

trace {
  fields = "task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes"
  file = "${params.output.dir}/trace.txt"
  raw = true
}

process {

  withLabel:retry {
    process.maxRetries = 3
    process.error_strategy = "ignore"
    errorStrategy = { "${task.attempt}" == "${task.max_retries}" ? "retry" : "${task.error_strategy}" }
  }

  withLabel:sratoolkit {
    container = 'systemsgenetics/sratoolkit:0.38'
    time = "48h"
    module = false
  }
  withLabel:python3scripts {
    container = 'systemsgenetics/python3scripts:1.0.0'
    time = "1h"
  }
  withLabel:fastqc {
    container = 'systemsgenetics/fastqc:0.11.7'
    time = "24h"
  }
  withLabel:trimmomatic {
    container = 'systemsgenetics/trimmomatic:0.38'
    time = "72h"
  }
  withLabel:hisat2 {
    container = 'systemsgenetics/hisat2:2.1.0'
    time = "48h"
  }
  withLabel:samtools {
    container = 'biocontainers/samtools:v1.2_cv3'
    time = "48h"
  }
  withLabel:stringtie {
    container = 'systemsgenetics/stringtie:1.3.4d'
    time = "48h"
  }
  withLabel:rate_limit {
    maxForks = 1
  }
}


profiles {
  standard {
    process {
      executor = "local"
    }
  }

  testing {
    process.error_strategy = "terminate"
    process.max_retries = 1
  }

  inDocker {
    docker {
        enabled = true
        fixOwnership = true
        runOptions = '-u $(id -u):$(id -g)'
    }
  }

  inSingularity {
    singularity {
        enabled = true
    }
  }

  //
  // Clemson's Palmetto cluster uses the PBS scheduler. Here we provide
  // an example for execution of this workflow on Palmetto with some
  // defaults for all steps of the workflow.
  //
  // Since the PBS resource list does not support disk or local scratch,
  // we must use a custom resource list in order to guarantee a certain
  // amount of local scratch space. In this case, we select nodes from phases
  // on Palmetto which have plenty of local scratch and tend to be highly
  // available. Additionally, nextflow does not properly support the "ncpus"
  // and "mem" options in PBS Professional, so we must specify these resources
  // using "clusterOptions" instead of the "cpus" and "mem" directives.
  //
  pbs {
    process {
      withLabel:sratoolkit {
        module = "sratoolkit"
      }
      withLabel:python3scripts {
        module = "python3"
      }
      withLabel:fastqc {
        module = "fastQC"
      }
      withLabel:trimmomatic {
        module = "trimmomatic"
      }
      withLabel:hisat2 {
        module = "hisat2"
      }
      withLabel:samtools {
        module = "samtools"
      }
      withLabel:stringtie {
        module = "stringtie"
      }

      executor = "pbs"
      time = "8h"
      scratch = true
      stageInMode = "copy"

      withLabel: "multithreaded" {
        clusterOptions = "-l select=1:phase=4:mem=8gb:ncpus=${params.execution.threads}"
      }
      withLabel: "!multithreaded" {
        clusterOptions = "-l select=1:phase=4:mem=2gb:ncpus=2"
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }

  //
  // WSU's Kamiak cluster uses the SLURM scheduler. Here we provide
  // an example for execution of this workflow on Kamiak with some
  // defaults for all steps of the workflow.
  //
  slurm {
    process {
      executor = "slurm"
      queue = {users queue}
      time = "4h"

      withLabel: "multithreaded" {
        cpus = "${params.execution.threads}"
      }
      withLabel: "!multithreaded" {
        cpus = 1
      }
    }
    executor {
      queueSize = "${params.execution.queue_size}"
    }
  }
}
